{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install seaborn\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "import sklearn\n",
    "import nltk.collocations \n",
    "from nltk import FreqDist, word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import string, re\n",
    "import urllib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_list = pd.read_csv('380lyrics.csv')\n",
    "charts = pd.read_csv('chart.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Rows with Other and Not Available Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_list.genre.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_list.drop(song_list[song_list.genre == \"Other\"].index, inplace = True)\n",
    "song_list.drop(song_list[song_list.genre == \"Not Available\"].index, inplace = True)\n",
    "song_list.drop(song_list[song_list.genre == \"Indie\"].index, inplace = True)\n",
    "song_list.drop(song_list[song_list.genre == \"Folk\"].index, inplace = True)\n",
    "song_list.drop(song_list[song_list.artist == \"dolcenera\"].index, inplace = True)\n",
    "song_list.drop(song_list[song_list.artist == \"brthhse-onkelz\"].index, inplace = True)\n",
    "song_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_list.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop rows with NAN values for column song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "song_list.dropna(inplace = True)\n",
    "song_list.song.isna().sum()\n",
    "song_list.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "song_list.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Song titles to remove dashes and capitalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_song_names(titles_list):\n",
    "    no_dot = list(map(lambda item: item.replace(\".\", \"\"), titles_list))\n",
    "    no_dash = list(map(lambda item: item.replace(\"-\", \" \"), no_dot))\n",
    "    return list(map(lambda item: string.capwords(item), no_dash))\n",
    "\n",
    "titles_list = song_list.song\n",
    "final_titles = clean_song_names(titles_list)\n",
    "len(final_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append Clean Song names to DF as a new column and drop old column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = song_list\n",
    "# new_df.head()\n",
    "new_titles = final_titles\n",
    "\n",
    "\n",
    "new_lyr = pd.DataFrame(new_titles)\n",
    "new_lyr.tail()\n",
    "\n",
    "final_df = new_df.join(new_lyr)\n",
    "\n",
    "final_df.drop(columns = ['song', 'level_0', \"index\"], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df.rename(columns = {0: \"song\"}, inplace = True)\n",
    "final_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Song titles with Nan Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.genre.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rock_df = final_df[final_df.genre == \"Rock\"][:2000]\n",
    "pop_df = final_df[final_df.genre == \"Pop\"][:2000]\n",
    "hip_df = final_df[final_df.genre == \"Hip-Hop\"][:2000]\n",
    "metal_df = final_df[final_df.genre == \"Metal\"][:2000]\n",
    "jazz_df = final_df[final_df.genre == \"Jazz\"][:2000]\n",
    "elec_df = final_df[final_df.genre == \"Electronic\"][:2000]\n",
    "country_df = final_df[final_df.genre == \"Country\"][:2000]\n",
    "rnb_df = final_df[final_df.genre == \"R&B\"][:2000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rock_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df.drop(final_df[final_df.genre == \"Rock\"].index, inplace = True)\n",
    "final_df.drop(final_df[final_df.genre == \"Pop\"].index, inplace = True)\n",
    "final_df.drop(final_df[final_df.genre == \"Hip-Hop\"].index, inplace = True)\n",
    "final_df.drop(final_df[final_df.genre == \"Metal\"].index, inplace = True)\n",
    "final_df.drop(final_df[final_df.genre == \"Jazz\"].index, inplace = True)\n",
    "final_df.drop(final_df[final_df.genre == \"Electronic\"].index, inplace = True)\n",
    "final_df.drop(final_df[final_df.genre == \"Country\"].index, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "maybe_df = final_df.append([rock_df, pop_df, hip_df, metal_df, jazz_df, elec_df, country_df])\n",
    "maybe_df.genre.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_df.drop(maybe_df[maybe_df.genre == \"R&B\"].index, inplace = True)\n",
    "maybe_df = maybe_df.append([rnb_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_df.genre.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_df.drop(columns = [\"index\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_list = list(maybe_df.lyrics)\n",
    "\n",
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "english = list(set(nltk.corpus.words.words()))\n",
    "\n",
    "def clean_docs_lemma(lyrics_list):\n",
    "    cleaned = []\n",
    "    pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "    for lyric in lyrics_list:\n",
    "        clean_lyric = nltk.regexp_tokenize(lyric, pattern)\n",
    "        lyric_lower = [i.lower() for i in clean_lyric]\n",
    "        stop_words = stopwords.words('english')\n",
    "        stopwords_list = stop_words + list(string.punctuation)\n",
    "        stopwords_list += [\"''\", '\"\"', '...', '``']\n",
    "        lyrics_tokens_stopped = [w for w in lyric_lower if not w in stopwords_list]\n",
    "        lyric_lemmas = [lemmatizer.lemmatize(word) for word in lyrics_tokens_stopped]\n",
    "        c = \" \".join(lyric_lemmas)\n",
    "        cleaned.append(c)\n",
    "    return cleaned\n",
    "\n",
    "def clean_docs_stemma(lyrics_list):\n",
    "    cleaned = []\n",
    "    pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "    for lyric in lyrics_list:\n",
    "        clean_lyric = nltk.regexp_tokenize(lyric, pattern)\n",
    "        lyric_lower = [i.lower() for i in clean_lyric]\n",
    "        stop_words = stopwords.words('english')\n",
    "        stopwords_list = stop_words + list(string.punctuation)\n",
    "        stopwords_list += [\"''\", '\"\"', '...', '``']\n",
    "        lyrics_tokens_stopped = [w for w in lyric_lower if not w in stopwords_list]\n",
    "        lyric_stemmas = [stemmer.stem(word) for word in lyrics_tokens_stopped]\n",
    "        c = \" \".join(lyric_stemmas)\n",
    "        cleaned.append(c)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First thing we wanted to do is test whether Lemmatizing works better than Stemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmed Lyrics Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmed_lyrics = clean_docs_lemma(lyrics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(set(nltk.word_tokenize(\" \".join(lemmed_lyrics))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = lemmed_lyrics\n",
    "y = maybe_df.genre\n",
    "\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 8000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Split Data in 3 pieces\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X1, X2, y1, y2 = train_test_split(X, y, test_size=0.5, random_state=18)\n",
    "        \n",
    "len(y1), len(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=18) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 1 - Train 3 weakest models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('count_vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      " ...obs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('count_vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      " ...m='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.3, n_estimators=50, random_state=None))])\n",
      "Pipeline(memory=None,\n",
      "     steps=[('count_vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      " ...ki',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform'))])\n",
      "Random Forest pipeline test accuracy: 0.354\n",
      "ADA pipeline test accuracy: 0.348\n",
      "KNN pipeline test accuracy: 0.151\n"
     ]
    }
   ],
   "source": [
    "# Train Weakest Models\n",
    "\n",
    "pipe_RF = Pipeline([('count_vectorizer', CountVectorizer()), \n",
    "                     ('tfidf_vectorizer', TfidfTransformer()),\n",
    "                     ('clf', RandomForestClassifier())\n",
    "                    ])\n",
    "\n",
    "pipe_ADA = Pipeline([('count_vectorizer', CountVectorizer()), \n",
    "                     ('tfidf_vectorizer', TfidfTransformer()),\n",
    "                     ('clf', AdaBoostClassifier(learning_rate=0.3))\n",
    "                    ])\n",
    "\n",
    "\n",
    "pipe_KNN = Pipeline([('count_vectorizer', CountVectorizer()), \n",
    "                     ('tfidf_vectorizer', TfidfTransformer()),\n",
    "                     ('clf', KNeighborsClassifier())\n",
    "                    ])\n",
    "\n",
    "# List of pipelines, List of pipeline names\n",
    "pipelines = [pipe_RF, pipe_ADA, pipe_KNN]\n",
    "pipeline_names = ['Random Forest', 'ADA', \"KNN\"]\n",
    "\n",
    "# Loop to fit each of the three pipelines\n",
    "for pipe in pipelines:\n",
    "    print(pipe)\n",
    "    pipe.fit(X1_train, y1_train)\n",
    "\n",
    "# Compare accuracies\n",
    "X1_scores = []\n",
    "for index, val in enumerate(pipelines):\n",
    "    tup = (pipeline_names[index], val.score(X1_test, y1_test), val.predict_proba(X1_train), val.predict(X1_train))\n",
    "    X1_scores.append(tup)\n",
    "    print('%s pipeline test accuracy: %.3f' % (pipeline_names[index], val.score(X1_test, y1_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Random Forest', 0.354375, array([[0. , 0.1, 0. , ..., 0.4, 0.3, 0.1],\n",
       "        [0. , 0. , 0.8, ..., 0.2, 0. , 0. ],\n",
       "        [0. , 0.1, 0. , ..., 0. , 0.1, 0. ],\n",
       "        ...,\n",
       "        [0.7, 0.1, 0. , ..., 0. , 0.2, 0. ],\n",
       "        [0.1, 0.8, 0. , ..., 0. , 0. , 0. ],\n",
       "        [0.8, 0. , 0. , ..., 0. , 0. , 0.1]]), array(['Pop', 'Hip-Hop', 'Metal', ..., 'Country', 'Electronic', 'Country'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creating Ensemble of 3 weakest learners - NB, Adaboost and KNN\n",
    "classes = val.classes_\n",
    "classes\n",
    "\n",
    "\n",
    "R1_AVG_Scores = (X1_scores[0][2] + X1_scores[1][2] + X1_scores[2][2])/3\n",
    "R1_df =  pd.DataFrame(R1_AVG_Scores, columns = [item + \"_AVG\" for item in classes])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=18) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 2 Train Good learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('count_vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      " ...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])\n",
      "Pipeline(memory=None,\n",
      "     steps=[('count_vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      " ...    subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False))])\n",
      "Multinomail NB pipeline test accuracy: 0.415\n",
      "Gboost pipeline test accuracy: 0.414\n"
     ]
    }
   ],
   "source": [
    "pipe_NB = Pipeline([('count_vectorizer', CountVectorizer()), \n",
    "                     ('tfidf_vectorizer', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())\n",
    "                    ])\n",
    "\n",
    "pipe_GBoost = Pipeline([('count_vectorizer', CountVectorizer()), \n",
    "                     ('tfidf_vectorizer', TfidfTransformer()),\n",
    "                     ('clf', GradientBoostingClassifier(learning_rate=0.3))\n",
    "                    ])\n",
    "\n",
    "# List of pipelines, List of pipeline names\n",
    "pipelines = [pipe_NB, pipe_GBoost]\n",
    "pipeline_names = ['Multinomail NB', \"Gboost\"]\n",
    "\n",
    "# Loop to fit each of the three pipelines\n",
    "for pipe in pipelines:\n",
    "    print(pipe)\n",
    "    pipe.fit(X2_train, y2_train)\n",
    "\n",
    "# Compare accuracies\n",
    "X2_scores = []\n",
    "for index, val in enumerate(pipelines):\n",
    "    tup = (pipeline_names[index], val.score(X2_test, y2_test), val.predict_proba(X2_train), val.predict(X2_train))\n",
    "    X2_scores.append(tup)\n",
    "    print('%s pipeline test accuracy: %.3f' % (pipeline_names[index], val.score(X2_test, y2_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Multinomail NB',\n",
       " 0.415,\n",
       " array([[0.06476146, 0.35241613, 0.06331353, ..., 0.13243585, 0.08613732,\n",
       "         0.10548437],\n",
       "        [0.1981774 , 0.07245812, 0.04322267, ..., 0.10622388, 0.18648187,\n",
       "         0.16461711],\n",
       "        [0.22100511, 0.08076004, 0.0505977 , ..., 0.12314752, 0.11177447,\n",
       "         0.0915945 ],\n",
       "        ...,\n",
       "        [0.19471283, 0.07157141, 0.03604731, ..., 0.09726023, 0.15274023,\n",
       "         0.16414313],\n",
       "        [0.11403141, 0.18961607, 0.15143814, ..., 0.09237297, 0.09056385,\n",
       "         0.10338478],\n",
       "        [0.0982226 , 0.19033551, 0.10705505, ..., 0.13630554, 0.1378852 ,\n",
       "         0.10853184]]),\n",
       " array(['Electronic', 'Country', 'Jazz', ..., 'Country', 'Electronic',\n",
       "        'Electronic'], dtype='<U10'))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_AVG_Scores = (X2_scores[0][2] + X2_scores[1][2])/2\n",
    "R2_df =  pd.DataFrame(R2_AVG_Scores, columns = [item + \"_AVG\" for item in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R1_df.shape == R2_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country_AVG</th>\n",
       "      <th>Electronic_AVG</th>\n",
       "      <th>Hip-Hop_AVG</th>\n",
       "      <th>Jazz_AVG</th>\n",
       "      <th>Metal_AVG</th>\n",
       "      <th>Pop_AVG</th>\n",
       "      <th>R&amp;B_AVG</th>\n",
       "      <th>Rock_AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.021688</td>\n",
       "      <td>0.077889</td>\n",
       "      <td>0.047074</td>\n",
       "      <td>0.077984</td>\n",
       "      <td>0.172722</td>\n",
       "      <td>0.381309</td>\n",
       "      <td>0.145245</td>\n",
       "      <td>0.076089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.014405</td>\n",
       "      <td>0.123191</td>\n",
       "      <td>0.404214</td>\n",
       "      <td>0.014668</td>\n",
       "      <td>0.093370</td>\n",
       "      <td>0.258538</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.030391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.042194</td>\n",
       "      <td>0.141628</td>\n",
       "      <td>0.039385</td>\n",
       "      <td>0.042362</td>\n",
       "      <td>0.440424</td>\n",
       "      <td>0.175590</td>\n",
       "      <td>0.075555</td>\n",
       "      <td>0.042863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.074281</td>\n",
       "      <td>0.109367</td>\n",
       "      <td>0.072252</td>\n",
       "      <td>0.041520</td>\n",
       "      <td>0.108542</td>\n",
       "      <td>0.208857</td>\n",
       "      <td>0.042202</td>\n",
       "      <td>0.342980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.276824</td>\n",
       "      <td>0.074027</td>\n",
       "      <td>0.037418</td>\n",
       "      <td>0.077080</td>\n",
       "      <td>0.273547</td>\n",
       "      <td>0.176094</td>\n",
       "      <td>0.042204</td>\n",
       "      <td>0.042805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country_AVG  Electronic_AVG  Hip-Hop_AVG  Jazz_AVG  Metal_AVG   Pop_AVG  \\\n",
       "0     0.021688        0.077889     0.047074  0.077984   0.172722  0.381309   \n",
       "1     0.014405        0.123191     0.404214  0.014668   0.093370  0.258538   \n",
       "2     0.042194        0.141628     0.039385  0.042362   0.440424  0.175590   \n",
       "3     0.074281        0.109367     0.072252  0.041520   0.108542  0.208857   \n",
       "4     0.276824        0.074027     0.037418  0.077080   0.273547  0.176094   \n",
       "\n",
       "    R&B_AVG  Rock_AVG  \n",
       "0  0.145245  0.076089  \n",
       "1  0.061224  0.030391  \n",
       "2  0.075555  0.042863  \n",
       "3  0.042202  0.342980  \n",
       "4  0.042204  0.042805  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country_AVG</th>\n",
       "      <th>Electronic_AVG</th>\n",
       "      <th>Hip-Hop_AVG</th>\n",
       "      <th>Jazz_AVG</th>\n",
       "      <th>Metal_AVG</th>\n",
       "      <th>Pop_AVG</th>\n",
       "      <th>R&amp;B_AVG</th>\n",
       "      <th>Rock_AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.047782</td>\n",
       "      <td>0.517785</td>\n",
       "      <td>0.035567</td>\n",
       "      <td>0.081856</td>\n",
       "      <td>0.058145</td>\n",
       "      <td>0.099964</td>\n",
       "      <td>0.065361</td>\n",
       "      <td>0.093540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.137993</td>\n",
       "      <td>0.057003</td>\n",
       "      <td>0.025317</td>\n",
       "      <td>0.192387</td>\n",
       "      <td>0.048993</td>\n",
       "      <td>0.110806</td>\n",
       "      <td>0.122537</td>\n",
       "      <td>0.304965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.136361</td>\n",
       "      <td>0.068976</td>\n",
       "      <td>0.029945</td>\n",
       "      <td>0.392138</td>\n",
       "      <td>0.096692</td>\n",
       "      <td>0.101011</td>\n",
       "      <td>0.080609</td>\n",
       "      <td>0.094267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.175803</td>\n",
       "      <td>0.081353</td>\n",
       "      <td>0.024063</td>\n",
       "      <td>0.201832</td>\n",
       "      <td>0.086792</td>\n",
       "      <td>0.206319</td>\n",
       "      <td>0.113154</td>\n",
       "      <td>0.110683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.066228</td>\n",
       "      <td>0.065306</td>\n",
       "      <td>0.043323</td>\n",
       "      <td>0.045614</td>\n",
       "      <td>0.512806</td>\n",
       "      <td>0.090862</td>\n",
       "      <td>0.072373</td>\n",
       "      <td>0.103488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country_AVG  Electronic_AVG  Hip-Hop_AVG  Jazz_AVG  Metal_AVG   Pop_AVG  \\\n",
       "0     0.047782        0.517785     0.035567  0.081856   0.058145  0.099964   \n",
       "1     0.137993        0.057003     0.025317  0.192387   0.048993  0.110806   \n",
       "2     0.136361        0.068976     0.029945  0.392138   0.096692  0.101011   \n",
       "3     0.175803        0.081353     0.024063  0.201832   0.086792  0.206319   \n",
       "4     0.066228        0.065306     0.043323  0.045614   0.512806  0.090862   \n",
       "\n",
       "    R&B_AVG  Rock_AVG  \n",
       "0  0.065361  0.093540  \n",
       "1  0.122537  0.304965  \n",
       "2  0.080609  0.094267  \n",
       "3  0.113154  0.110683  \n",
       "4  0.072373  0.103488  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "R3_df = pd.concat([R1_df, R2_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = pd.concat([y1_train, y2_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12800,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6400, 6400)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y3[0:6400] == y1_train), sum(y3[6400:] == y2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 3 - Plug all AVG probabilities as features and train a final NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class labels:\n",
      "['Country', 'Electronic', 'Hip-Hop', 'Jazz', 'Metal', 'Pop', 'R&B', 'Rock']\n",
      "\n",
      "\n",
      "New product labels:\n",
      "[5 2 4 ... 4 1 1]\n",
      "\n",
      "\n",
      "One hot labels; 7 binary columns, one for each of the categories.\n",
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n",
      "\n",
      "\n",
      "One hot labels shape:\n",
      "(12800, 8)\n"
     ]
    }
   ],
   "source": [
    "#Converting y_test to categorical\n",
    "\n",
    "product = y3\n",
    "\n",
    "le = preprocessing.LabelEncoder() #Initialize. le used as abbreviation fo label encoder\n",
    "le.fit(product)\n",
    "print(\"Original class labels:\")\n",
    "print(list(le.classes_))\n",
    "print('\\n')\n",
    "product_cat = le.transform(product)  \n",
    "#list(le.inverse_transform([0, 1, 3, 3, 0, 6, 4])) #If you wish to retrieve the original descriptive labels post production\n",
    "\n",
    "print('New product labels:')\n",
    "print(product_cat)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "print('One hot labels; 7 binary columns, one for each of the categories.') #Each row will be all zeros except for the category for that observation.\n",
    "product_onehot = to_categorical(product_cat)\n",
    "print(product_onehot)\n",
    "print('\\n')\n",
    "\n",
    "print('One hot labels shape:')\n",
    "print(np.shape(product_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7200, 8) (1800, 8) (7200, 8) (1800, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3_train, X3_test, y3_train, y3_test = train_test_split(R3_df[:9000], product_onehot[:9000], test_size=0.2, random_state=123)  \n",
    "\n",
    "# X_train = X_train.reset_index(drop=True)\n",
    "# y_train = y_train.reset_index(drop=True)  \n",
    "\n",
    "\n",
    "print(X3_train.shape, X3_test.shape, y3_train.shape, y3_test.shape)\n",
    "y3_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate NN mmodel\n",
    "\n",
    "random.seed(123)\n",
    "from keras import models\n",
    "from keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(7, input_dim=8, kernel_initializer='normal', activation='tanh')) #2 hidden layers\n",
    "model.add(layers.Dense(3, activation='tanh'))\n",
    "model.add(layers.Dense(8, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200 samples, validate on 1800 samples\n",
      "Epoch 1/50\n",
      "7200/7200 [==============================] - 1s 71us/step - loss: 2.0675 - acc: 0.3221 - val_loss: 2.0618 - val_acc: 0.4339\n",
      "Epoch 2/50\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 2.0555 - acc: 0.4379 - val_loss: 2.0497 - val_acc: 0.4450\n",
      "Epoch 3/50\n",
      "7200/7200 [==============================] - 0s 30us/step - loss: 2.0428 - acc: 0.4042 - val_loss: 2.0366 - val_acc: 0.4589\n",
      "Epoch 4/50\n",
      "7200/7200 [==============================] - 0s 29us/step - loss: 2.0287 - acc: 0.4597 - val_loss: 2.0220 - val_acc: 0.4583\n",
      "Epoch 5/50\n",
      "7200/7200 [==============================] - 0s 34us/step - loss: 2.0127 - acc: 0.4426 - val_loss: 2.0051 - val_acc: 0.4567\n",
      "Epoch 6/50\n",
      "7200/7200 [==============================] - 0s 28us/step - loss: 1.9941 - acc: 0.4535 - val_loss: 1.9854 - val_acc: 0.4511\n",
      "Epoch 7/50\n",
      "7200/7200 [==============================] - 0s 24us/step - loss: 1.9724 - acc: 0.4486 - val_loss: 1.9622 - val_acc: 0.4411\n",
      "Epoch 8/50\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 1.9470 - acc: 0.4426 - val_loss: 1.9350 - val_acc: 0.4356\n",
      "Epoch 9/50\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 1.9175 - acc: 0.4351 - val_loss: 1.9038 - val_acc: 0.4450\n",
      "Epoch 10/50\n",
      "7200/7200 [==============================] - 0s 26us/step - loss: 1.8837 - acc: 0.4460 - val_loss: 1.8681 - val_acc: 0.4367\n",
      "Epoch 11/50\n",
      "7200/7200 [==============================] - 0s 26us/step - loss: 1.8459 - acc: 0.4344 - val_loss: 1.8289 - val_acc: 0.4406\n",
      "Epoch 12/50\n",
      "7200/7200 [==============================] - 0s 26us/step - loss: 1.8048 - acc: 0.4425 - val_loss: 1.7866 - val_acc: 0.4444\n",
      "Epoch 13/50\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 1.7612 - acc: 0.4546 - val_loss: 1.7422 - val_acc: 0.4467\n",
      "Epoch 14/50\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 1.7163 - acc: 0.4583 - val_loss: 1.6968 - val_acc: 0.4567\n",
      "Epoch 15/50\n",
      "7200/7200 [==============================] - ETA: 0s - loss: 1.6732 - acc: 0.468 - 0s 28us/step - loss: 1.6709 - acc: 0.4686 - val_loss: 1.6514 - val_acc: 0.4633\n",
      "Epoch 16/50\n",
      "7200/7200 [==============================] - 0s 24us/step - loss: 1.6254 - acc: 0.4821 - val_loss: 1.6060 - val_acc: 0.4911\n",
      "Epoch 17/50\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 1.5798 - acc: 0.5074 - val_loss: 1.5604 - val_acc: 0.5217\n",
      "Epoch 18/50\n",
      "7200/7200 [==============================] - 0s 26us/step - loss: 1.5342 - acc: 0.5515 - val_loss: 1.5148 - val_acc: 0.5844\n",
      "Epoch 19/50\n",
      "7200/7200 [==============================] - 0s 27us/step - loss: 1.4884 - acc: 0.5943 - val_loss: 1.4694 - val_acc: 0.6150\n",
      "Epoch 20/50\n",
      "7200/7200 [==============================] - 0s 26us/step - loss: 1.4427 - acc: 0.6208 - val_loss: 1.4245 - val_acc: 0.6367\n",
      "Epoch 21/50\n",
      "7200/7200 [==============================] - 0s 27us/step - loss: 1.3974 - acc: 0.6415 - val_loss: 1.3793 - val_acc: 0.6583\n",
      "Epoch 22/50\n",
      "7200/7200 [==============================] - 0s 29us/step - loss: 1.3528 - acc: 0.6560 - val_loss: 1.3359 - val_acc: 0.6706\n",
      "Epoch 23/50\n",
      "7200/7200 [==============================] - 0s 32us/step - loss: 1.3097 - acc: 0.6725 - val_loss: 1.2947 - val_acc: 0.6672\n",
      "Epoch 24/50\n",
      "7200/7200 [==============================] - 0s 28us/step - loss: 1.2686 - acc: 0.6806 - val_loss: 1.2551 - val_acc: 0.6944\n",
      "Epoch 25/50\n",
      "7200/7200 [==============================] - 0s 26us/step - loss: 1.2300 - acc: 0.6907 - val_loss: 1.2177 - val_acc: 0.6917\n",
      "Epoch 26/50\n",
      "7200/7200 [==============================] - 0s 26us/step - loss: 1.1935 - acc: 0.6951 - val_loss: 1.1827 - val_acc: 0.7100\n",
      "Epoch 27/50\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 1.1593 - acc: 0.7062 - val_loss: 1.1507 - val_acc: 0.7161\n",
      "Epoch 28/50\n",
      "7200/7200 [==============================] - 0s 27us/step - loss: 1.1275 - acc: 0.7164 - val_loss: 1.1200 - val_acc: 0.7333\n",
      "Epoch 29/50\n",
      "7200/7200 [==============================] - 0s 27us/step - loss: 1.0976 - acc: 0.7236 - val_loss: 1.0912 - val_acc: 0.7383\n",
      "Epoch 30/50\n",
      "7200/7200 [==============================] - 0s 30us/step - loss: 1.0696 - acc: 0.7325 - val_loss: 1.0647 - val_acc: 0.7433\n",
      "Epoch 31/50\n",
      "7200/7200 [==============================] - 0s 33us/step - loss: 1.0432 - acc: 0.7400 - val_loss: 1.0392 - val_acc: 0.7533\n",
      "Epoch 32/50\n",
      "7200/7200 [==============================] - 0s 27us/step - loss: 1.0185 - acc: 0.7478 - val_loss: 1.0152 - val_acc: 0.7606\n",
      "Epoch 33/50\n",
      "7200/7200 [==============================] - 0s 29us/step - loss: 0.9949 - acc: 0.7601 - val_loss: 0.9934 - val_acc: 0.7656\n",
      "Epoch 34/50\n",
      "7200/7200 [==============================] - 0s 32us/step - loss: 0.9724 - acc: 0.7668 - val_loss: 0.9716 - val_acc: 0.7750\n",
      "Epoch 35/50\n",
      "7200/7200 [==============================] - 0s 33us/step - loss: 0.9514 - acc: 0.7746 - val_loss: 0.9510 - val_acc: 0.7833\n",
      "Epoch 36/50\n",
      "7200/7200 [==============================] - 0s 34us/step - loss: 0.9314 - acc: 0.7828 - val_loss: 0.9319 - val_acc: 0.7878\n",
      "Epoch 37/50\n",
      "7200/7200 [==============================] - 0s 31us/step - loss: 0.9120 - acc: 0.7875 - val_loss: 0.9131 - val_acc: 0.7917\n",
      "Epoch 38/50\n",
      "7200/7200 [==============================] - 0s 33us/step - loss: 0.8935 - acc: 0.7931 - val_loss: 0.8956 - val_acc: 0.7978\n",
      "Epoch 39/50\n",
      "7200/7200 [==============================] - 0s 33us/step - loss: 0.8761 - acc: 0.7969 - val_loss: 0.8786 - val_acc: 0.8000\n",
      "Epoch 40/50\n",
      "7200/7200 [==============================] - 0s 32us/step - loss: 0.8593 - acc: 0.8004 - val_loss: 0.8631 - val_acc: 0.8022\n",
      "Epoch 41/50\n",
      "7200/7200 [==============================] - 0s 34us/step - loss: 0.8435 - acc: 0.8060 - val_loss: 0.8481 - val_acc: 0.8089\n",
      "Epoch 42/50\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 0.8282 - acc: 0.8058 - val_loss: 0.8327 - val_acc: 0.8111\n",
      "Epoch 43/50\n",
      "7200/7200 [==============================] - 0s 34us/step - loss: 0.8135 - acc: 0.8075 - val_loss: 0.8187 - val_acc: 0.8133\n",
      "Epoch 44/50\n",
      "7200/7200 [==============================] - 0s 31us/step - loss: 0.7994 - acc: 0.8108 - val_loss: 0.8074 - val_acc: 0.8200\n",
      "Epoch 45/50\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 0.7861 - acc: 0.8157 - val_loss: 0.7926 - val_acc: 0.8200\n",
      "Epoch 46/50\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 0.7731 - acc: 0.8186 - val_loss: 0.7806 - val_acc: 0.8194\n",
      "Epoch 47/50\n",
      "7200/7200 [==============================] - 0s 26us/step - loss: 0.7608 - acc: 0.8203 - val_loss: 0.7687 - val_acc: 0.8228\n",
      "Epoch 48/50\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 0.7489 - acc: 0.8251 - val_loss: 0.7577 - val_acc: 0.8250\n",
      "Epoch 49/50\n",
      "7200/7200 [==============================] - 0s 26us/step - loss: 0.7378 - acc: 0.8246 - val_loss: 0.7482 - val_acc: 0.8328\n",
      "Epoch 50/50\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 0.7270 - acc: 0.8282 - val_loss: 0.7367 - val_acc: 0.8283\n"
     ]
    }
   ],
   "source": [
    "model_val = model.fit(X3_train,\n",
    "                    y3_train,\n",
    "                    epochs=50,\n",
    "                    batch_size=48,\n",
    "                    validation_data=(X3_test, y3_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do final Test with 'Holdout' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3800/3800 [==============================] - 0s 12us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7857534348337274, 0.8039473685465361]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_score = model.evaluate(R3_df[9000:], product_onehot[9000:])\n",
    "validation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict_classes(R3_df[9000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Electronic',\n",
       " 'R&B',\n",
       " 'Electronic',\n",
       " 'Electronic',\n",
       " 'Hip-Hop',\n",
       " 'Metal',\n",
       " 'Jazz',\n",
       " 'Pop',\n",
       " 'Electronic',\n",
       " 'Metal']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tryit = list(le.inverse_transform(y_hat))\n",
    "tryit[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 3, 1, 4, 6, 6, 4, 4, 1, 6])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 3, 1, 4, 6, 1, 4, 4, 1, 1]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual = [np.where(item == 1)[0][0] for item in product_onehot[9000:]]\n",
    "actual[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rock',\n",
       " 'Jazz',\n",
       " 'Electronic',\n",
       " 'Metal',\n",
       " 'R&B',\n",
       " 'Electronic',\n",
       " 'Metal',\n",
       " 'Metal',\n",
       " 'Electronic',\n",
       " 'Electronic']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_genre = list(le.inverse_transform(actual))\n",
    "actual_genre[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for item in list(zip(tryit, actual_genre)):\n",
    "        if item[0] == item[1]:\n",
    "            count += 1\n",
    "            \n",
    "count/len(tryit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model.evaluate(X_train, y_train)\n",
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = model.evaluate(X_test, y_test)\n",
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = model.predict(X_test)\n",
    "result = y_test - y_hat_test\n",
    "print(sum(sum(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(lemmed_lyrics, maybe_df.genre, test_size=0.2, random_state=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modles_lem = [item[0] for item in lemmed_basic_scores]\n",
    "accuracy_lem = [item[1] for item in lemmed_basic_scores]\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "n_groups = 5\n",
    "\n",
    "means_men = (20, 35, 30, 35, 27)\n",
    "std_men = (2, 3, 4, 1, 2)\n",
    "\n",
    "means_women = (25, 32, 34, 20, 25)\n",
    "std_women = (3, 5, 2, 3, 3)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 8))\n",
    "\n",
    "\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.35\n",
    "\n",
    "opacity = 0.5\n",
    "error_config = {'ecolor': '0.3'}\n",
    "\n",
    "lemmed_bars = ax.bar(index, accuracy_lem, bar_width,\n",
    "                alpha=opacity, color='b',\n",
    "                label='Lematized')\n",
    "\n",
    "stemmed_bars = ax.bar(index + bar_width, accuracy_stem, bar_width,\n",
    "                alpha=opacity, color='r',\n",
    "                label='Stemmatized')\n",
    "\n",
    "ax.set_xlabel('Model Type', fontsize = 14)\n",
    "ax.set_ylabel('Accuracy Scores', fontsize = 14)\n",
    "ax.set_title('Stemmed vs. Lemmed Accuracy Score Comparison', fontsize = 18)\n",
    "ax.set_xticks(index + bar_width / 2)\n",
    "ax.set_xticklabels(modles_lem)\n",
    "\n",
    "plt.axhline(y=1/len(set(y)), color='#17CA83', linestyle='-', label = \"Random Guessing\")\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We decided to pick Lemmatized over Stemmatized and top three models for further optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we want to try using PCA to improve performance and reduce dimentionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tfidf = TfidfVectorizer()\n",
    "# response = tfidf.fit_transform(lemmed_lyrics)\n",
    "\n",
    "# PCA_df = pd.DataFrame(response.toarray(), columns=tfidf.get_feature_names())\n",
    "# PCA_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA = response  # this comes from above where you're vectorizing tdif dictionary\n",
    "\n",
    "# non_zero_cols = DATA.nnz / float(DATA.shape[0])\n",
    "# print(\"Average Number of Non-Zero Elements in Vectorized Articles: {}\".format(non_zero_cols))\n",
    "\n",
    "# percent_sparse = 1 - (non_zero_cols / float(DATA.shape[1]))\n",
    "# print('Percentage of columns containing 0: {}'.format(percent_sparse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PCA_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Features table and Target table and testing first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_pca = PCA_df\n",
    "# y_pca = maybe_df.genre\n",
    "\n",
    "# len(X_pca) == len(y_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split  \n",
    "# X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y_pca, test_size=0.2, random_state=18) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try different PCA values and pick a number that preserves sufficient % of variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# pca_1 = PCA(n_components=500)\n",
    "# pca_2 = PCA(n_components=1000)\n",
    "# pca_3 = PCA(n_components=1455)\n",
    "# pca_4 = PCA(n_components=2000)\n",
    "\n",
    "# principalComponents = pca_1.fit_transform(X_pca)\n",
    "# principalComponents = pca_2.fit_transform(X_pca)\n",
    "# principalComponents = pca_3.fit_transform(X_pca)\n",
    "# principalComponents = pca_4.fit_transform(X_pca)\n",
    "\n",
    "# print(np.sum(pca_1.explained_variance_ratio_))\n",
    "# print(np.sum(pca_2.explained_variance_ratio_))\n",
    "# print(np.sum(pca_3.explained_variance_ratio_))\n",
    "# print(np.sum(pca_4.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will test PCA witn n = 1800 on our top 3 models to see if it helps performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pipe_NB_pca = Pipeline([('pca', PCA(n_components=3000, random_state=18)),\n",
    "#                      ('clf', GaussianNB())\n",
    "#                     ])\n",
    "\n",
    "# pipe_RF_pca = Pipeline([('pca', PCA(n_components=3000, random_state=18)),\n",
    "#                      ('clf', RandomForestClassifier(n_jobs = -1))\n",
    "#                    ])\n",
    "                  \n",
    "# pipe_GBoost_pca = Pipeline([('pca', PCA(n_components=1800, random_state=18)),\n",
    "#                      ('clf', GradientBoostingClassifier(learning_rate=0.3))\n",
    "#                     ])\n",
    "\n",
    "\n",
    "# # List of pipelines, List of pipeline names\n",
    "# pipelines = [pipe_NB_pca, pipe_RF_pca, pipe_GBoost_pca]\n",
    "# pipeline_names = ['Multinomial NB', \"Random Forest\", \"Gradient Boost\"]\n",
    "\n",
    "# # Loop to fit each of the three pipelines\n",
    "# for pipe in pipelines:\n",
    "#     print(pipe)\n",
    "#     pipe.fit(X_train_pca, y_train_pca)\n",
    "\n",
    "# # Compare accuracies\n",
    "# PCA_scores = []\n",
    "# for index, val in enumerate(pipelines):\n",
    "#     tup = (pipeline_names[index], val.score(X_test_pca, y_test_pca))\n",
    "#     lemmed_basic_scores.append(tup)\n",
    "#     print('%s pipeline test accuracy: %.3f' % (pipeline_names[index], val.score(X_test_pca, y_test_pca)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA1800_results = dict(Multinomail_NB = 0.203,\n",
    "# Gradient_boost = 0.422,\n",
    "# Random_forest =0.290)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# n_groups = 3\n",
    "\n",
    "# basic_mod_name = modles_lem[:3]\n",
    "# basic_mod_acc = accuracy_lem[:3]\n",
    "# pca_models_acc = [item[1] for item in PCA1800_results.items()]\n",
    "\n",
    "# fig, ax = plt.subplots(figsize = (10,8))\n",
    "\n",
    "\n",
    "# index = np.arange(n_groups)\n",
    "# bar_width = 0.35\n",
    "\n",
    "# opacity = 0.5\n",
    "# error_config = {'ecolor': '0.3'}\n",
    "\n",
    "# basic_bars = ax.bar(index, basic_mod_acc, bar_width,\n",
    "#                 alpha=opacity, color='b',\n",
    "#                 label='Basic Model')\n",
    "\n",
    "# pca_bars = ax.bar(index + bar_width, pca_models_acc , bar_width,\n",
    "#                 alpha=opacity, color='r',\n",
    "#                 label='PCA n_components = 1800')\n",
    "\n",
    "# ax.set_xlabel('Model Type', fontsize = 14)\n",
    "# ax.set_ylabel('Accuracy Scores', fontsize = 14)\n",
    "# ax.set_title('Basic Model vs. PCA with n = 1800 Model Comparison', fontsize = 18)\n",
    "# ax.set_xticks(index + bar_width / 2)\n",
    "# ax.set_xticklabels(basic_mod_name)\n",
    "\n",
    "# plt.axhline(y=1/len(set(y)), color='#17CA83', linestyle='-', label = \"Random Guessing\")\n",
    "# ax.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We decided that it's not worth using PCA for our models because it increases computational time and doesn't really improve performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use GridSearch to try to optimize our  top 3 models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top3_pipelines = [pipe_NB, pipe_GBoost, pipe_RF]\n",
    "Top3_pipeline_names = ['Multinomail NB', \"Gboost\", 'Random Forest']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GS_pipe_RF = Pipeline([('count_vectorizer', CountVectorizer()), \n",
    "                     ('tfidf_vectorizer', TfidfTransformer()),\n",
    "                     ('clf', RandomForestClassifier())\n",
    "                    ])\n",
    "\n",
    "sorted(GS_pipe_RF.get_params().keys())\n",
    "\n",
    "rf_param_grid = dict(clf__n_estimators = [10, 30, 100], clf__criterion = ['gini', 'entropy'], \n",
    "                    clf__max_depth = [2, 6, 10], clf__min_samples_split = [5, 10],\n",
    "                    clf__min_samples_leaf = [3, 6])\n",
    "\n",
    "\n",
    "gs_RF = GridSearchCV(estimator=GS_pipe_RF,\n",
    "            param_grid=rf_param_grid,\n",
    "            scoring='accuracy',\n",
    "            cv=3)\n",
    "\n",
    "gs_RF.fit(X_train, y_train)\n",
    "\n",
    "dt_gs_training_score = np.mean(gs_RF.cv_results_['mean_train_score'])\n",
    "dt_gs_testing_score = gs_RF.score(X_test, y_test)\n",
    "\n",
    "print(\"Mean Training Score: {:.4}%\".format(dt_gs_training_score * 100))\n",
    "print(\"Mean Testing Score: {:.4}%\".format(dt_gs_testing_score * 100))\n",
    "print(\"Best Parameter Combination Found During Grid Search: {}\".format(gs_RF.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grid_RF_metrics = dict(train_score = dt_gs_training_score, test_score = dt_gs_testing_score, best_params = gs_RF.best_params_)\n",
    "Grid_RF_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch Gboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "GS_pipe_GB = Pipeline([('count_vectorizer', CountVectorizer()), \n",
    "                     ('tfidf_vectorizer', TfidfTransformer()),\n",
    "                     ('clf', GradientBoostingClassifier())\n",
    "                    ])\n",
    "# sorted(GS_pipe_RF.get_params().keys())\n",
    "\n",
    "GB_params = {\n",
    "    \"clf__learning_rate\": [0.2, 0.25],\n",
    "    \"clf__min_samples_split\": [4, 5],\n",
    "    \"clf__min_samples_leaf\": [6],\n",
    "    \"clf__max_depth\":[3],\n",
    "    \"clf__n_estimators\":[100, 150]\n",
    "    }\n",
    "\n",
    "gs_GB = GridSearchCV(estimator=GS_pipe_GB,\n",
    "            param_grid=GB_params,\n",
    "            scoring='accuracy',\n",
    "            cv=3)\n",
    "\n",
    "gs_GB.fit(X_train, y_train)\n",
    "\n",
    "dt_GB_training_score = np.mean(gs_GB.cv_results_['mean_train_score'])\n",
    "dt_GB_testing_score = gs_GB.score(X_test, y_test)\n",
    "\n",
    "print(\"Mean Training Score: {:.4}%\".format(dt_GB_training_score * 100))\n",
    "print(\"Mean Testing Score: {:.4}%\".format(dt_GB_testing_score * 100))\n",
    "print(\"Best Parameter Combination Found During Grid Search: {}\".format(gs_GB.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Grid_GB_metrics = dict(train_score = dt_GB_training_score, test_score = dt_GB_testing_score, best_params = gs_GB.best_params_)\n",
    "Grid_GB_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hip_lyric = [\"dropin my dough real quick. Data Science squad for the win. Drake got nothin on us!\"]\n",
    "jazz_lyric = [\"humdinger, babababoo, bababaa from san francisco to georgia, we teach you to code like wah wah\"]\n",
    "rock_lyric = ['when I was young I thought code is not important. Now I learned that i need to know it if I want to grow.']\n",
    "def test_genre(lyric):\n",
    "    lemmed_test = clean_docs_lemma(lyric)\n",
    "    print(\"This song is definetely {}!\".format(gs_GB.predict(lemmed_test)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_genre(rock_lyric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_top3 = [(\"GradientBoost with GridSearch\", Grid_GB_metrics['test_score']), basic_scores[0], (\"Random Forest with GridSearch\", basic_scores[2][1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_top3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_model = [item[0] for item in final_top3]\n",
    "top3_scores = [item[1] for item in final_top3]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Top3 Models Final Performance\", color ='#061152' , fontsize = 20)\n",
    "plt.ylabel(\"Accuracy Score\", color = '#061152', fontsize = 16)\n",
    "plt.bar(top3_model, top3_scores, color = \"#17CA83\", label = \"Top 3 Models\")\n",
    "\n",
    "plt.axhline(y=1/len(set(y)), color='#AF2138', linestyle='-', label = \"Random Guessing\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_mode_df = pd.DataFrame.from_dict(Grid_GB_metrics['best_params'])\n",
    "top_mode_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "test = random.sample(stemmed_lyrics, 7000)\n",
    "lemmed_lyr = [nltk.word_tokenize(doc) for doc in test]\n",
    "\n",
    "lemmed_lyr\n",
    "# test\n",
    "dictionary = gensim.corpora.Dictionary(lemmed_lyr)\n",
    "\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in lemmed_lyr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Topic Classifier using BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=15, id2word=dictionary, passes=2, workers=2)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Topic Classifier Using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=15, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tfidf[bow_corpus[12]]\n",
    "\n",
    "# Get terms from the dictionary and pair with weights\n",
    "\n",
    "weights = [(dictionary[pair[0]], pair[1]) for pair in weights]\n",
    "weights[-35:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Initialize the word cloud\n",
    "\n",
    "d = {}\n",
    "for a, x in weights:\n",
    "    d[a] = x\n",
    "    \n",
    "wc = WordCloud(\n",
    "    background_color=\"white\",\n",
    "    max_words=2000,\n",
    "    width = 1024,\n",
    "    height = 720,\n",
    "    stopwords=stopwords.words(\"english\")\n",
    ")\n",
    "\n",
    "# Generate the cloud\n",
    "\n",
    "wc.generate_from_frequencies(d)\n",
    "plt.figure(figsize = (12, 8))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
